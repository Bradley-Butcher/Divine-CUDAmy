

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>The Second Circle: Adding (to the Pain) &#8212; The Divine CUDAmy</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'part_I/chapter3';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The Third Circle: Speaking with the Python" href="chapter4.html" />
    <link rel="prev" title="The First Circle: Blocks and Threads" href="chapter2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../introduction.html">
  
  
  
  
  
    <p class="title logo__title">The Divine CUDAmy</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../introduction.html">
                    The Divine CUDAmy
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: The Inferno</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html"><em>The Dark Wood of Errors</em>: Installing CUDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html"><em>The First Circle</em>: Blocks and Threads</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#"><em>The Second Circle</em>: Adding (to the Pain)</a></li>

<li class="toctree-l1"><a class="reference internal" href="chapter4.html">The Third Circle: Speaking with the Python</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fpart_I/chapter3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/part_I/chapter3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The Second Circle: Adding (to the Pain)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><em>The Second Circle</em>: Adding (to the Pain)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-kernel"><strong>2.1 The Kernel</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-the-kernel"><strong>2.2 Calling the Kernel</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-to-vectors"><strong>2.3 Extending to Vectors</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-the-vector-addition-kernel"><strong>2.4 Calling the Vector Addition Kernel</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-to-matrices"><strong>2.5 Extending to Matrices</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-the-matrix-addition-kernel"><strong>2.6 Calling the Matrix Addition Kernel</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#avoiding-sweet-but-unneccesary-pain-with-cuda-best-practices">Avoiding sweet but unneccesary pain with CUDA best practices</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-checking-it-s-not-paranoia-if-they-re-really-out-to-get-you">Error Checking: It’s Not Paranoia If They’re Really Out to Get You</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-optimal-block-and-grid-sizes-it-s-not-about-size-it-s-how-you-use-it">Choosing Optimal Block and Grid Sizes: It’s Not About Size, It’s How You Use It</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-allocation-and-de-allocation-give-and-take-but-mostly-take">Memory Allocation and De-allocation: Give and Take, But Mostly Take</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#const-correctness-some-things-never-change">Const Correctness: Some Things Never Change</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="the-second-circle-adding-to-the-pain">
<h1><em>The Second Circle</em>: Adding (to the Pain)<a class="headerlink" href="#the-second-circle-adding-to-the-pain" title="Permalink to this heading">#</a></h1>
<p>Welcome, dear reader, to the Second Circle of our CUDA Inferno. If you’ve made it this far, congratulations! You’ve either successfully installed CUDA and set up your environment, or you’ve decided to skip the first chapter entirely and dive headfirst into the deep end. Either way, I salute your bravery (or foolhardiness).</p>
<p>In this chapter, we’re going to tackle the “Hello, World!” of CUDA programming: a simple addition program. Yes, you heard that right. We’re going to use your GPU, a device so computationally intensive you could grill a steak on it, to add numbers. It’s like using a supercomputer to play tic-tac-toe, but bear with me. This simple program will serve as a gentle introduction to the basics of CUDA programming, and it will set the stage for the more complex and exciting programs we’ll tackle in the later chapters.</p>
<section id="the-kernel">
<h2><strong>2.1 The Kernel</strong><a class="headerlink" href="#the-kernel" title="Permalink to this heading">#</a></h2>
<p>The heart of any CUDA program is the kernel. This is the function that gets executed on the GPU. In CUDA, we define a kernel using the <code class="docutils literal notranslate"><span class="pre">__global__</span></code> keyword. Here’s what our addition kernel looks like:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">add</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="o">*</span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This kernel takes two integers, <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>, and a pointer to an integer <code class="docutils literal notranslate"><span class="pre">c</span></code>. It adds <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> together and stores the result in <code class="docutils literal notranslate"><span class="pre">c</span></code>.</p>
</section>
<section id="calling-the-kernel">
<h2><strong>2.2 Calling the Kernel</strong><a class="headerlink" href="#calling-the-kernel" title="Permalink to this heading">#</a></h2>
<p>To call our kernel, we use the triple angle bracket syntax <code class="docutils literal notranslate"><span class="pre">&lt;&lt;&lt;</span> <span class="pre">&gt;&gt;&gt;</span></code>. This syntax allows us to specify the number of blocks and threads we want to use. For our simple addition program, we only need one block and one thread:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">c</span><span class="p">;</span><span class="w"> </span><span class="c1">// host copy of c</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">d_c</span><span class="p">;</span><span class="w"> </span><span class="c1">// device copy of c</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Allocate space for device copy of c</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_c</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Call the kernel</span>
<span class="w">    </span><span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="n">d_c</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Copy result back to host</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">d_c</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Cleanup</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_c</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this code, we first allocate space on the GPU for our result using <code class="docutils literal notranslate"><span class="pre">cudaMalloc</span></code>. We then call our kernel. After the kernel has finished executing, we copy the result back to the host using <code class="docutils literal notranslate"><span class="pre">cudaMemcpy</span></code>. Finally, we clean up by freeing the memory we allocated on the GPU using <code class="docutils literal notranslate"><span class="pre">cudaFree</span></code>.</p>
</section>
<section id="extending-to-vectors">
<h2><strong>2.3 Extending to Vectors</strong><a class="headerlink" href="#extending-to-vectors" title="Permalink to this heading">#</a></h2>
<p>Now that we’ve mastered the art of adding two numbers together, let’s raise the stakes a bit. Let’s add two vectors together. Here’s what our vector addition kernel looks like:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">addVectors</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">index</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">c</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this kernel, <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code> are pointers to our vectors, and <code class="docutils literal notranslate"><span class="pre">N</span></code> is the size of the vectors. We calculate the</p>
<p>…index of the current thread using <code class="docutils literal notranslate"><span class="pre">threadIdx.x</span></code>, and we use this index to access the corresponding elements in <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">threadIdx.x</span></code> is a built-in variable in CUDA that gives the index of the current thread within a block. Since we’re dealing with vectors, we only need one dimension, hence the <code class="docutils literal notranslate"><span class="pre">.x</span></code>.</p>
<p>The beauty of CUDA is that it allows us to perform operations on multiple elements simultaneously. Each thread operates independently and performs its operation on a different element of the vector. This is why we don’t need to write any loops in our kernel. The loop is implicit: one iteration per thread.</p>
</section>
<section id="calling-the-vector-addition-kernel">
<h2><strong>2.4 Calling the Vector Addition Kernel</strong><a class="headerlink" href="#calling-the-vector-addition-kernel" title="Permalink to this heading">#</a></h2>
<p>Calling our vector addition kernel is a bit more involved than calling our simple addition kernel. We need to allocate space on the GPU for our vectors, copy our vectors from the host to the GPU, call our kernel, and then copy the result back to the host. Here’s what that looks like:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">20</span><span class="p">;</span><span class="w"> </span><span class="c1">// size of vectors</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">;</span><span class="w"> </span><span class="c1">// host copies of a, b, c</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">d_a</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">d_b</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">d_c</span><span class="p">;</span><span class="w"> </span><span class="c1">// device copies of a, b, c</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Allocate space for device copies of a, b, c</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_b</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_c</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Setup input values</span>
<span class="w">    </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span><span class="w"> </span><span class="n">fill_array</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span><span class="w"> </span><span class="n">fill_array</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Copy inputs to device</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_b</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Call the kernel</span>
<span class="w">    </span><span class="n">addVectors</span><span class="o">&lt;&lt;&lt;</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span><span class="w"> </span><span class="n">d_b</span><span class="p">,</span><span class="w"> </span><span class="n">d_c</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Copy result back to host</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">d_c</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Cleanup</span>
<span class="w">    </span><span class="n">free</span><span class="p">(</span><span class="n">a</span><span class="p">);</span><span class="w"> </span><span class="n">free</span><span class="p">(</span><span class="n">b</span><span class="p">);</span><span class="w"> </span><span class="n">free</span><span class="p">(</span><span class="n">c</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_a</span><span class="p">);</span><span class="w"> </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_b</span><span class="p">);</span><span class="w"> </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_c</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="extending-to-matrices">
<h2><strong>2.5 Extending to Matrices</strong><a class="headerlink" href="#extending-to-matrices" title="Permalink to this heading">#</a></h2>
<p>Adding two matrices together is a natural extension of adding two vectors together. Instead of one index, we now have two indices: one for the row and one for the column. Here’s what our matrix addition kernel looks like:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">addMatrices</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">row</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this kernel, <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code> are pointers to our matrices, and <code class="docutils literal notranslate"><span class="pre">N</span></code> is the size of the matrices. We calculate the row and column indices using <code class="docutils literal notranslate"><span class="pre">blockIdx</span></code>, <code class="docutils literal notranslate"><span class="pre">blockDim</span></code>, and <code class="docutils literal notranslate"><span class="pre">threadIdx</span></code>, and we use these indices to access the corresponding elements in <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">blockIdx</span></code> and <code class="docutils literal notranslate"><span class="pre">blockDim</span></code> are built-in variables in CUDA that give the index of the current block within the grid and the dimensions of the block, respectively. By multiplying the block index by the block dimension and adding the thread index, we can calculate a unique index for each</p>
<p>…thread across all blocks. This is how we can use <code class="docutils literal notranslate"><span class="pre">blockIdx</span></code>, <code class="docutils literal notranslate"><span class="pre">blockDim</span></code>, and <code class="docutils literal notranslate"><span class="pre">threadIdx</span></code> to index into our matrices.</p>
<p>Let’s break it down with an example. Suppose we have a grid of blocks, where each block is of size 16x16 (so <code class="docutils literal notranslate"><span class="pre">blockDim.x</span> <span class="pre">=</span> <span class="pre">blockDim.y</span> <span class="pre">=</span> <span class="pre">16</span></code>), and we have 4 blocks in the x-direction and 3 blocks in the y-direction. If we’re in the thread of <code class="docutils literal notranslate"><span class="pre">blockIdx.x</span> <span class="pre">=</span> <span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">blockIdx.y</span> <span class="pre">=</span> <span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">threadIdx.x</span> <span class="pre">=</span> <span class="pre">5</span></code>, and <code class="docutils literal notranslate"><span class="pre">threadIdx.y</span> <span class="pre">=</span> <span class="pre">3</span></code>, the unique row and column indices for this thread would be:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span><span class="w"> </span><span class="c1">// 1 * 16 + 3 = 19</span>
<span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"> </span><span class="c1">// 2 * 16 + 5 = 37</span>
</pre></div>
</div>
<p>So, this thread would be responsible for the element at the 20th row and 38th column of the matrix (remember, indices are 0-based).</p>
<p>Just like with vector addition, each thread operates independently and performs its operation on a different element of the matrix. This is why we don’t need to write any loops in our kernel. The loop is implicit: one iteration per thread.</p>
</section>
<section id="calling-the-matrix-addition-kernel">
<h2><strong>2.6 Calling the Matrix Addition Kernel</strong><a class="headerlink" href="#calling-the-matrix-addition-kernel" title="Permalink to this heading">#</a></h2>
<p>Calling our matrix addition kernel is similar to calling our vector addition kernel. We need to allocate space on the GPU for our matrices, copy our matrices from the host to the GPU, call our kernel, and then copy the result back to the host. Here’s what that looks like:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">10</span><span class="p">;</span><span class="w"> </span><span class="c1">// size of matrices</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">;</span><span class="w"> </span><span class="c1">// host copies of a, b, c</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">d_a</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">d_b</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">d_c</span><span class="p">;</span><span class="w"> </span><span class="c1">// device copies of a, b, c</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Allocate space for device copies of a, b, c</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_b</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_c</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Setup input values</span>
<span class="w">    </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span><span class="w"> </span><span class="n">fill_matrix</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span><span class="w"> </span><span class="n">fill_matrix</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Copy inputs to device</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_b</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Call the kernel</span>
<span class="w">    </span><span class="n">dim3</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">);</span>
<span class="w">    </span><span class="n">dim3</span><span class="w"> </span><span class="n">numBlocks</span><span class="p">(</span><span class="n">N</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">y</span><span class="p">);</span>
<span class="w">    </span><span class="n">addMatrices</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span><span class="w"> </span><span class="n">d_b</span><span class="p">,</span><span class="w"> </span><span class="n">d_c</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Copy result back to host</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">d_c</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Cleanup</span>
<span class="w">    </span><span class="n">free</span><span class="p">(</span><span class="n">a</span><span class="p">);</span><span class="w"> </span><span class="n">free</span><span class="p">(</span><span class="n">b</span><span class="p">);</span><span class="w"> </span><span class="n">free</span><span class="p">(</span><span class="n">c</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_a</span><span class="p">);</span><span class="w"> </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_b</span><span class="p">);</span><span class="w"> </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_c</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="avoiding-sweet-but-unneccesary-pain-with-cuda-best-practices">
<h1>Avoiding sweet but unneccesary pain with CUDA best practices<a class="headerlink" href="#avoiding-sweet-but-unneccesary-pain-with-cuda-best-practices" title="Permalink to this heading">#</a></h1>
<p>Isn’t adding great? We’ve created something truly special, flawless. Truly cast-iron in its design. Or so we’d like to think. In the blissful ignorance of missing best practices, our program can become akin to a sinking ship, leaking memory from small unseen cracks while mismanaging thread blocks like a lost conductor in a grand symphony. But fear not, for I have compiled a list of best practices, which I have just googled (remember, I’m new to this too) to help you avoid the same mistakes I made. You may skip this section, as I probably would, but do so at your own peril. I salute you, you brave, idiotic soul.</p>
<section id="error-checking-it-s-not-paranoia-if-they-re-really-out-to-get-you">
<h2>Error Checking: It’s Not Paranoia If They’re Really Out to Get You<a class="headerlink" href="#error-checking-it-s-not-paranoia-if-they-re-really-out-to-get-you" title="Permalink to this heading">#</a></h2>
<p>First and foremost, CUDA doesn’t like to tell you when things go wrong. It’ll sit in silence, maybe smirk a bit, and watch you pull your hair out in confusion. That’s why it’s paramount to check for errors for CUDA API calls and kernel launches. I’ve done you a favor and included a handy macro and function to assist with this. It’s a like a smoke detector. When it’s silent, you can enjoy the soothing hum of your GPU. When it goes off, it’s time to evacuate your code.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">gpuAssert</span><span class="p">(</span><span class="n">cudaError_t</span><span class="w"> </span><span class="n">code</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">line</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">abort</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">   </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">code</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">cudaSuccess</span><span class="p">)</span><span class="w"> </span>
<span class="w">   </span><span class="p">{</span>
<span class="w">      </span><span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">&quot;GPUassert: %s %s %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">code</span><span class="p">),</span><span class="w"> </span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">line</span><span class="p">);</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">abort</span><span class="p">)</span><span class="w"> </span><span class="n">exit</span><span class="p">(</span><span class="n">code</span><span class="p">);</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>So, what is a macro, you ask? In C and C++, a macro is a way of defining shorthand for other code. It’s like when your girlfriend whispers in your ear to remind you of her parents names. You just have to say gpuErrchk and the compiler understands it as gpuAssert.</p>
<p>The #define directive defines the gpuErrchk(ans) macro. When this macro is used, it’s as if the code inside the curly brackets { gpuAssert((ans), <strong>FILE</strong>, <strong>LINE</strong>); } is copied and pasted in its place. This bit of magic invokes the gpuAssert function with three arguments: a CUDA operation that returns a cudaError_t, the current file name <strong>FILE</strong>, and the current line number <strong>LINE</strong>.</p>
<p>The gpuAssert fucntion takes a cudaError_t code, file name, line number, and whether to abort if an error is found. If the cudaError_t code indicates an error, the function prints an error message with the name of the file and the line number where the error happened, adding a personal touch to your debugging experience. If the abort parameter is true, the program stops there. This is the equivalent of someone yelling “Stop the presses!” when a typo is found in an old-timey newspaper.</p>
<p>So let’s say you’ve called a CUDA function like so: <code class="docutils literal notranslate"><span class="pre">gpuErrchk(cudaMalloc((void</span> <span class="pre">**)&amp;d_a,</span> <span class="pre">size));</span></code>. If cudaMalloc encounters an error and doesn’t return cudaSuccess, gpuErrchk will call gpuAssert which will print out an error message telling you exactly where the error occurred. Without this careful error checking, CUDA might lead you on a wild goose chase around your code, cackling in the shadows as you wonder why your program is crashing or producing incorrect results. Error checking can save you from such torment. So, take a moment to appreciate these lines of code, for they will be your steadfast allies in the turbulent seas of CUDA programming.</p>
</section>
<section id="choosing-optimal-block-and-grid-sizes-it-s-not-about-size-it-s-how-you-use-it">
<h2>Choosing Optimal Block and Grid Sizes: It’s Not About Size, It’s How You Use It<a class="headerlink" href="#choosing-optimal-block-and-grid-sizes-it-s-not-about-size-it-s-how-you-use-it" title="Permalink to this heading">#</a></h2>
<p>In our example, we’ve been launching an excessive number of blocks, each with a single thread. You may find this satisfying in its simplicity, but unfortunately, it’s akin to buying a sports car and never shifting out of first gear.</p>
<p>You see, CUDA cores enjoy company. They perform their best when surrounded by threads, not blocks. Instead, aim for fewer blocks, but pack them full of threads. Like stuffing clowns into a car, the more threads you fit in a block, the more efficiently you’re using your resources. Common choices range from 256 to 512 threads per block, though you’ll have to gauge the clown-car capacity of your specific GPU.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">THREADS_PER_BLOCK</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">BLOCKS</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">THREADS_PER_BLOCK</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">THREADS_PER_BLOCK</span><span class="p">;</span>
<span class="w">    </span><span class="p">...</span>
<span class="w">    </span><span class="n">dim3</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="p">(</span><span class="n">THREADS_PER_BLOCK</span><span class="p">,</span><span class="w"> </span><span class="n">THREADS_PER_BLOCK</span><span class="p">);</span>
<span class="w">    </span><span class="n">dim3</span><span class="w"> </span><span class="nf">numBlocks</span><span class="p">(</span><span class="n">BLOCKS</span><span class="p">,</span><span class="w"> </span><span class="n">BLOCKS</span><span class="p">);</span>
<span class="w">    </span><span class="n">addMatrices</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span><span class="w"> </span><span class="n">d_b</span><span class="p">,</span><span class="w"> </span><span class="n">d_c</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
</pre></div>
</div>
<p>In this piece of code, that I, your humble guide, have painstakingly assembled, we’ve filled each block with threads until they’re practically bulging at the seams. This is courtesy of the variable <code class="docutils literal notranslate"><span class="pre">THREADS_PER_BLOCK</span></code> set to 16 – a decision driven by the undeniable charm of powers of 2, and maybe a dash of mathematical superstition.</p>
<p>These threads are then herded into blocks like overzealous party-goers crammed into a small room. When we set the kernel in motion with addMatrices&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);, we’re essentially opening the door to the party and hoping nobody notices the fire code violation. Here, numBlocks and threadsPerBlock are dim3 variables, a dim3 type being the CUDA equivalent of an overzealous party planner, making sure even one-dimensional parties are technically ready to go 3D at a moment’s notice. In our case, numBlocks takes care of the number of blocks per dimension, and threadsPerBlock is the bouncer, keeping a check on the number of threads in each block.</p>
</section>
<section id="memory-allocation-and-de-allocation-give-and-take-but-mostly-take">
<h2>Memory Allocation and De-allocation: Give and Take, But Mostly Take<a class="headerlink" href="#memory-allocation-and-de-allocation-give-and-take-but-mostly-take" title="Permalink to this heading">#</a></h2>
<p>When it comes to memory, CUDA is a bit of a hoarder. It wants its own separate memory, separate from what the CPU uses. This memory needs to be allocated and then freed once we’re done with it. If malloc or cudaMalloc fails, it returns a null pointer. That’s like the bank declining your credit card. It’s embarrassing, it’s problematic, and it’s something you need to check for. Always verify your allocations, or you’ll be dining on a fine plate of segmentation faults for dinner.</p>
</section>
<section id="const-correctness-some-things-never-change">
<h2>Const Correctness: Some Things Never Change<a class="headerlink" href="#const-correctness-some-things-never-change" title="Permalink to this heading">#</a></h2>
<p>As a final note, let’s talk about ‘const’. In our kernel, there’s a parameter ‘N’ that doesn’t get modified. It’s like the grumpy old man of parameters, stuck in his ways. When you come across such parameters, declare them as ‘const’. It helps the compiler, and it signals your intentions to anyone reading your code.</p>
<p>And there you have it, dear reader. You’ve successfully navigated the Second Circle of our CUDA Inferno. You’ve learned how to write and call CUDA kernels, and you’ve even learned how to add vectors and matrices together. I hope you’re feeling proud of yourself, because you should be. In the next chapter, we’ll learn how to interface our CUDA programs with Python, and things will start to get a lot more interesting. So hang in there. The best is yet to come.</p>
<p>And remember, if you’re feeling overwhelmed, just take a deep breath and remember that every expert was once a beginner. Even me. Especially me. If you’re feeling like you’re in over your head, just remember: it’s not rocket science. Well, unless you’re using CUDA for rocket science. In which case, it is. But don’t worry, you’re smart. You wouldn’t be here if you weren’t. Or maybe you’re just masochistic. Either way, I like your style.</p>
<p>As always, the full code can be found in the /code folder. Except when it can’t, then it won’t be there.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./part_I"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="chapter2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><em>The First Circle</em>: Blocks and Threads</p>
      </div>
    </a>
    <a class="right-next"
       href="chapter4.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Third Circle: Speaking with the Python</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><em>The Second Circle</em>: Adding (to the Pain)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-kernel"><strong>2.1 The Kernel</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-the-kernel"><strong>2.2 Calling the Kernel</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-to-vectors"><strong>2.3 Extending to Vectors</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-the-vector-addition-kernel"><strong>2.4 Calling the Vector Addition Kernel</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-to-matrices"><strong>2.5 Extending to Matrices</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-the-matrix-addition-kernel"><strong>2.6 Calling the Matrix Addition Kernel</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#avoiding-sweet-but-unneccesary-pain-with-cuda-best-practices">Avoiding sweet but unneccesary pain with CUDA best practices</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-checking-it-s-not-paranoia-if-they-re-really-out-to-get-you">Error Checking: It’s Not Paranoia If They’re Really Out to Get You</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-optimal-block-and-grid-sizes-it-s-not-about-size-it-s-how-you-use-it">Choosing Optimal Block and Grid Sizes: It’s Not About Size, It’s How You Use It</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-allocation-and-de-allocation-give-and-take-but-mostly-take">Memory Allocation and De-allocation: Give and Take, But Mostly Take</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#const-correctness-some-things-never-change">Const Correctness: Some Things Never Change</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Bradley Butcher
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>